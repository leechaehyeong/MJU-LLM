{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'install'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
      "��ġ ������ �ƴմϴ�.\n"
     ]
    }
   ],
   "source": [
    "!install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.2.5-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.2.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.30-cp311-cp311-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.9.5-cp311-cp311-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.7 (from langchain)\n",
      "  Downloading langchain_core-0.2.8-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.79-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2,>=1 (from langchain)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.0/61.0 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting pydantic<3,>=1 (from langchain)\n",
      "  Downloading pydantic-2.7.4-py3-none-any.whl.metadata (109 kB)\n",
      "     ---------------------------------------- 0.0/109.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 109.4/109.4 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-8.4.1-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading multidict-6.0.5-cp311-cp311-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading yarl-1.9.4-cp311-cp311-win_amd64.whl.metadata (32 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.7->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\이채형\\desktop\\lmmtest\\sherry\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.7->langchain) (24.1)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.5-cp311-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.9/50.9 kB ? eta 0:00:00\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.4 (from pydantic<3,>=1->langchain)\n",
      "  Downloading pydantic_core-2.18.4-cp311-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\이채형\\desktop\\lmmtest\\sherry\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
      "  Downloading charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\이채형\\desktop\\lmmtest\\sherry\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\이채형\\desktop\\lmmtest\\sherry\\lib\\site-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.0.3-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain-0.2.5-py3-none-any.whl (974 kB)\n",
      "   ---------------------------------------- 0.0/974.6 kB ? eta -:--:--\n",
      "   --------------------- ----------------- 542.7/974.6 kB 11.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  972.8/974.6 kB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 974.6/974.6 kB 8.8 MB/s eta 0:00:00\n",
      "Downloading langchain_community-0.2.5-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.2 MB 14.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.9/2.2 MB 11.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.2 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.2 MB 10.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 10.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 9.4 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.5-cp311-cp311-win_amd64.whl (370 kB)\n",
      "   ---------------------------------------- 0.0/370.8 kB ? eta -:--:--\n",
      "   --------------------------------------  368.6/370.8 kB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 370.8/370.8 kB 7.7 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading langchain_core-0.2.8-py3-none-any.whl (315 kB)\n",
      "   ---------------------------------------- 0.0/315.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 315.8/315.8 kB 9.5 MB/s eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
      "Downloading langsmith-0.1.79-py3-none-any.whl (125 kB)\n",
      "   ---------------------------------------- 0.0/125.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 125.3/125.3 kB 7.2 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/15.8 MB 15.2 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 1.0/15.8 MB 12.6 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.5/15.8 MB 11.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 2.0/15.8 MB 11.8 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.5/15.8 MB 11.6 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.8/15.8 MB 11.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.3/15.8 MB 11.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.6/15.8 MB 10.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 4.1/15.8 MB 10.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.5/15.8 MB 10.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 5.0/15.8 MB 10.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 5.0/15.8 MB 10.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 5.0/15.8 MB 10.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 5.0/15.8 MB 10.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 5.0/15.8 MB 10.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 5.0/15.8 MB 10.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 5.1/15.8 MB 6.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.2/15.8 MB 6.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 5.6/15.8 MB 6.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 6.1/15.8 MB 6.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.5/15.8 MB 7.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.9/15.8 MB 7.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.3/15.8 MB 7.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.7/15.8 MB 7.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.2/15.8 MB 7.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.5/15.8 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 9.0/15.8 MB 7.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 9.4/15.8 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 10.0/15.8 MB 7.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 10.3/15.8 MB 7.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 10.6/15.8 MB 7.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 11.0/15.8 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.4/15.8 MB 7.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.8/15.8 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.3/15.8 MB 7.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.7/15.8 MB 7.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.1/15.8 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.6/15.8 MB 7.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.0/15.8 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.5/15.8 MB 7.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.0/15.8 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.8 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 9.2 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.7.4-py3-none-any.whl (409 kB)\n",
      "   ---------------------------------------- 0.0/409.0 kB ? eta -:--:--\n",
      "   --------------------------------------  399.4/409.0 kB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 409.0/409.0 kB 12.5 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.18.4-cp311-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.4/1.9 MB 13.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.9/1.9 MB 14.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.5/1.9 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 10.2 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp311-cp311-win_amd64.whl (144 kB)\n",
      "   ---------------------------------------- 0.0/144.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 144.7/144.7 kB 9.0 MB/s eta 0:00:00\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.9/64.9 kB ? eta 0:00:00\n",
      "Downloading SQLAlchemy-2.0.30-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.1 MB 15.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.1 MB 13.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.5/2.1 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 10.2 MB/s eta 0:00:00\n",
      "Downloading tenacity-8.4.1-py3-none-any.whl (27 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.8/60.8 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl (99 kB)\n",
      "   ---------------------------------------- 0.0/99.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 99.9/99.9 kB 5.6 MB/s eta 0:00:00\n",
      "Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.5/50.5 kB ? eta 0:00:00\n",
      "Downloading greenlet-3.0.3-cp311-cp311-win_amd64.whl (292 kB)\n",
      "   ---------------------------------------- 0.0/292.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 292.8/292.8 kB 9.1 MB/s eta 0:00:00\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 0.0/49.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 49.2/49.2 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Downloading orjson-3.10.5-cp311-none-win_amd64.whl (141 kB)\n",
      "   ---------------------------------------- 0.0/141.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 141.3/141.3 kB 8.2 MB/s eta 0:00:00\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "   ---------------------------------------- 0.0/121.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 121.4/121.4 kB 7.0 MB/s eta 0:00:00\n",
      "Downloading yarl-1.9.4-cp311-cp311-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.7/76.7 kB 4.4 MB/s eta 0:00:00\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: urllib3, tenacity, PyYAML, pydantic-core, orjson, numpy, mypy-extensions, multidict, marshmallow, jsonpointer, greenlet, frozenlist, charset-normalizer, attrs, annotated-types, yarl, typing-inspect, SQLAlchemy, requests, pydantic, jsonpatch, aiosignal, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.30 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 attrs-23.2.0 charset-normalizer-3.3.2 dataclasses-json-0.6.7 frozenlist-1.4.1 greenlet-3.0.3 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.5 langchain-community-0.2.5 langchain-core-0.2.8 langchain-text-splitters-0.2.1 langsmith-0.1.79 marshmallow-3.21.3 multidict-6.0.5 mypy-extensions-1.0.0 numpy-1.26.4 orjson-3.10.5 pydantic-2.7.4 pydantic-core-2.18.4 requests-2.32.3 tenacity-8.4.1 typing-inspect-0.9.0 urllib3-2.2.2 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 저는 인공지능 어시스턴트입니다. 도움이 필요하시면 언제든지 말씀해주세요. 어떻게 도와드릴 수 있을까요?\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "\n",
    "response = ollama.chat(model='qwen2:1.5b', messages=[\n",
    "    {'role': 'system', 'content': '당신은 유용한 인공지능 어시스턴트입니다.'},\n",
    "    {'role': 'user', 'content': '안녕하세요. 자기소개 부탁합니다.'}\n",
    "])\n",
    "\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능의 지도학습은 인공지능 모델을 학습시키는 과정입니다. 이 과정에서는 데이터를 사용하여 모델에 학습된 정보를 추가하고, 이를 통해 모델이 새로운 데이터에 대해 더 잘 예측할 수 있도록 합니다.\n",
      "\n",
      "인공지능의 지도학습은 두 가지 주요한 방법으로 이루어집니다: 트레이닝과 테스트입니다. 트레이닝 과정에서는 모델을 학습시키는 데 필요한 정보를 사용하여 모델을 학습시킵니다. 이때, 데이터가 얼마나 정확하게 모델에 반영되는지 확인합니다.\n",
      "\n",
      "테스트 과정에서는 모델이 학습된 정보를 사용하여 새로운 데이터에 대해 예측하는 능력을 검증합니다. 이렇게 하면 모델의 성능을 평가하고 필요한 부분을 수정할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "#local ollama inference\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434/\") #http://127.0.0.1:11434\n",
    "result = llm.invoke(\"인공지능의 지도학습이란?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='당신은 인공지능 전문가입니다. 다음의 질문에 답해주세요. : {input}'))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 다음의 질문에 답해주세요. : {input}\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'비지도 학습은 학습을 통해 새로운 정보를 배우는 과정에서, 학습자가 그 정보를 직접적으로 이해하고 적용하는 것을 의미합니다. 이는 일반적인 학습과 달리, 학습자는 정보를 직접적으로 받아들이지 않고, 정보가 어떻게 작동되는지를 이해하고 적용하는 것입니다.\\n\\n비지도 학습은 인공지능에 대한 학습을 포함할 수 있습니다. 예를 들어, 인공지능 모델이 새로운 데이터셋에서 학습하면, 그들은 데이터의 특징과 관계를 이해하고 이를 활용하여 새로운 문제를 해결합니다. 이는 일반적인 학습과 달리, 학습자가 직접적으로 정보를 받아들이지 않고, 정보가 어떻게 작동되는지를 이해하고 적용하는 것입니다.\\n\\n비지도 학습은 인공지능의 학습 과정에서 중요한 역할을 합니다. 이를 통해 인공지능 모델이 새로운 데이터셋에서 학습하여 더 정확한 예측 결과를 제공하거나, 더 효과적인 의사결정을 내리는 데 도움이 됩니다.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "result = chain.invoke({\"input\": \"비지도 학습이 무엇인가요?\"})\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural Language Processing (NLP)는 인간이 말하는 언어를 이해하고 그 의미를 분석하여 컴퓨터가 이해하도록 도와주는 기술입니다. 이 기술은 다양한 분야에서 사용되며, 예를 들어 인터넷 검색, 음성 인식, 텍스트 메시지 처리 등에 적용됩니다.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain1 = prompt | llm | output_parser\n",
    "\n",
    "\n",
    "chain1.invoke({\"input\": \"자연어 처리란 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LMM은 \"Learning Machine Model\"의 약어로, 학습 모델을 의미합니다. 이는 컴퓨터 프로그래밍에서 일반적으로 사용되는 알고리즘 중 하나입니다. LMM는 데이터를 분석하고 학습하여 새로운 데이터에 대한 예측을 수행하는 능력을 가지고 있습니다. 이는 인공지능(AI) 및 머신 러닝(Machine Learning) 분야에서 중요한 역할을 합니다.\\n                                             '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "\n",
    "\n",
    "# Chain1\n",
    "llm1 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434/\") #http://127.0.0.1:11434\n",
    "prompt1 = ChatPromptTemplate.from_messages([(\"system\",\"당신은 인공지능 전문가입니다. 사용자의 질문에 답변해주세요. \"),\n",
    "                                            (\"user\",\" {input} \")])\n",
    "\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "\n",
    "# Chain2 / 2 input\n",
    "question = input(\"질문: \")\n",
    "lang = input(\"언어를 선택해주세요.\")\n",
    "llm2 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434/\") #http://127.0.0.1:11434\n",
    "prompt2 = ChatPromptTemplate.from_messages([(\"system\",\"\"\"#Instruction\n",
    "                                           당신은 번역가입니다. 사용자가 입력한 문장을 {lang}로 번역해주세요.\n",
    "                                           당신은 결과를 말할 때, 원문과 번역결과를 반드시 함꼐 말해주세요.\n",
    "                                           \"\"\"),\n",
    "                                            (\"user\",\"\"\"{text}\n",
    "                                           \n",
    "                                              #Result                                        \n",
    "                                            번역문:\n",
    "                                            원문 :\n",
    "                                             \"\"\")])\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = RunnableMap({\"text\": RunnableLambda(lambda x: chain1), \"lang\": RunnableLambda(lambda x: lang)}) | prompt2 | llm2 | output_parser\n",
    "chain2.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lmm이 뭐야?\n",
      "영어\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LMM stands for \"Learning Machine Model,\" which refers to a type of algorithm used in computer programming, specifically designed to provide an accurate prediction based on specific data it has been trained on.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate,  HumanMessagePromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "\n",
    "\n",
    "# Chain1\n",
    "llm1 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434/\") #http://127.0.0.1:11434\n",
    "prompt1 = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 사용자의 질문에 답해주세요.\"),\n",
    "            HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "                                            ])\n",
    "output_parser = StrOutputParser()\n",
    "chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "\n",
    "# Chain2 / 2 input\n",
    "question = input(\"질문: \")\n",
    "lang = input(\"언어를 선택해주세요.\")\n",
    "llm2 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434/\") #http://127.0.0.1:11434\n",
    "prompt2 = ChatPromptTemplate.from_messages([SystemMessagePromptTemplate.from_template(\"\"\"\n",
    "                                            # Instruction\n",
    "                                            당신은 번역가입니다. 다음에 주어진 문장을 {lang}로 번역해주세요.                                                                                      \n",
    "                                            \"\"\"),\n",
    "                                            HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "                                            # Text\n",
    "                                            {text}                                          \n",
    "                                            # Result\"\"\")])\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = RunnableMap({\"text\": RunnableLambda(lambda x: chain1), \"lang\": RunnableLambda(lambda x: lang)}) | prompt2 | llm2 | output_parser\n",
    "print(question)\n",
    "print(lang)\n",
    "chain2.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chains(question, lemg):\n",
    "    # Chain1\n",
    "    llm1 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434/\") #http://127.0.0.1:11434\n",
    "    prompt1 = ChatPromptTemplate.from_messages([\n",
    "                SystemMessagePromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 사용자의 질문에 답해주세요.\"),\n",
    "                HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "                                                ])\n",
    "    output_parser = StrOutputParser()\n",
    "    chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "\n",
    "    # Chain2 / 2 input\n",
    "    question = input(\"질문: \")\n",
    "    lang = input(\"언어를 선택해주세요.\")\n",
    "    llm2 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434/\") #http://127.0.0.1:11434\n",
    "    prompt2 = ChatPromptTemplate.from_messages([SystemMessagePromptTemplate.from_template(\"\"\"\n",
    "    \n",
    "                                                                                                                                    # Instruction\n",
    "                                                당신은 번역가입니다. 다음에 주어진 문장을 {lang}로 번역해주세요.                                                                                      \n",
    "                                                \"\"\"),\n",
    "                                                HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "                                                # Text\n",
    "                                                {text}                                          \n",
    "                                                # Result\"\"\")])\n",
    "    output_parser = StrOutputParser()\n",
    "    chain2 = RunnableMap({\"text\": RunnableLambda(lambda x: chain1), \"lang\": RunnableLambda(lambda x: lang)}) | prompt2 | llm2 | output_parser\n",
    "    print(question)\n",
    "    print(lang)\n",
    "    chain2.invoke({\"input\": question})\n",
    "\n",
    "    result = chain.invoke({\"input\":  question})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatOllama' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_chains\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m파이썬이 뭐야? 20자로 짧게 설명해줘\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m영어\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m, in \u001b[0;36mrun_chains\u001b[1;34m(question, lemg)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_chains\u001b[39m(question, lemg):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Chain1\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     llm1 \u001b[38;5;241m=\u001b[39m \u001b[43mChatOllama\u001b[49m(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen2:1.5b\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:11434/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m#http://127.0.0.1:11434\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     prompt1 \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages([\n\u001b[0;32m      5\u001b[0m                 SystemMessagePromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m당신은 인공지능 전문가입니다. 사용자의 질문에 답해주세요.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      6\u001b[0m                 HumanMessagePromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m                                                 ])\n\u001b[0;32m      8\u001b[0m     output_parser \u001b[38;5;241m=\u001b[39m StrOutputParser()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ChatOllama' is not defined"
     ]
    }
   ],
   "source": [
    "run_chains(\"파이썬이 뭐야? 20자로 짧게 설명해줘\", \"영어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clac(x,y):\n",
    "    z= x+y\n",
    "    return z\n",
    "\n",
    "calc(1,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sherry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
